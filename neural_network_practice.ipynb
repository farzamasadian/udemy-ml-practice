{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30be99e",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a4214",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38909189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606801a",
   "metadata": {},
   "source": [
    "## checking if gpu is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40942377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c607e90",
   "metadata": {},
   "source": [
    "## benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976c3fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Num GPUs Available:  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x3048cfd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x3048cfd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 18:41:26.268926: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-05-06 18:41:26.269072: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-05-06 18:41:26.269080: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-05-06 18:41:26.269439: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-06 18:41:26.269460: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x3048cfd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x3048cfd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function normalize_img at 0x3048cfd80> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x3048cfd80>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 18:41:26.857085: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 10/469\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.3822 - loss: 1.8688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimport tensorflow as tf\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mimport tensorflow_datasets as tfds\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTensorFlow version:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, tf.__version__)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNum GPUs Available: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m, len(tf.config.experimental.list_physical_devices(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mGPU\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m)))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mtf.config.list_physical_devices(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mGPU\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m(ds_train, ds_test), ds_info = tfds.load(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mmnist\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    split=[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mtrain\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mtest\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    shuffle_files=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    as_supervised=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    with_info=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mdef normalize_img(image, label):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNormalizes images: `uint8` -> `float32`.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  return tf.cast(image, tf.float32) / 255., label\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mbatch_size = 128\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_train = ds_train.map(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_train = ds_train.cache()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_train = ds_train.shuffle(ds_info.splits[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mtrain\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m].num_examples)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_train = ds_train.batch(batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_test = ds_test.map(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_test = ds_test.batch(batch_size)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_test = ds_test.cache()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mmodel = tf.keras.models.Sequential([\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                 activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mrelu\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m                 activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mrelu\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#   tf.keras.layers.Dropout(0.25),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.Flatten(),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.Dense(128, activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mrelu\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m#   tf.keras.layers.Dropout(0.5),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m  tf.keras.layers.Dense(10, activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43msoftmax\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mmodel.compile(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    loss=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43msparse_categorical_crossentropy\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    optimizer=tf.keras.optimizers.Adam(0.001),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    metrics=[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43maccuracy\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mmodel.fit(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    ds_train,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    epochs=12,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    validation_data=ds_test,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1395\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m expr_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1394\u001b[39m         code_2 = \u001b[38;5;28mself\u001b[39m.shell.compile(expr_val, source, \u001b[33m'\u001b[39m\u001b[33meval\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         out = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1397\u001b[39m     \u001b[38;5;28mself\u001b[39m.shell.showtraceback()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:45\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=12,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b976d54",
   "metadata": {},
   "source": [
    "## TensorFlow Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0c4102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello World'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caaeefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465fba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello World')\n",
    "x =  tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7627409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'> \n",
      " <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(f'{type(x)} \\n {type(hello)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bf9da",
   "metadata": {},
   "source": [
    "## deprecated\n",
    "tf.session --> no need for sessions\n",
    "tf.placeholder --> tf.Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d02df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(initial_value=0, dtype=tf.int32, name='x')\n",
    "y = tf.Variable(initial_value=0, dtype=tf.int32, name='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26257fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4553d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial x: 5\n",
      "Initial y: 10\n",
      "Sum (initial): 15\n",
      "Updated x: 20\n",
      "Updated y: 15\n",
      "Sum (updated): 35\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize TensorFlow Variables\n",
    "x = tf.Variable(initial_value=5, dtype=tf.int32, name='x')\n",
    "y = tf.Variable(initial_value=10, dtype=tf.int32, name='y')\n",
    "\n",
    "print(f\"Initial x: {x.numpy()}\")\n",
    "print(f\"Initial y: {y.numpy()}\")\n",
    "\n",
    "# Perform the first addition\n",
    "sum_result_1 = tf.add(x, y)\n",
    "print(f\"Sum (initial): {sum_result_1.numpy()}\")\n",
    "\n",
    "# Update the value of x\n",
    "x.assign(20)\n",
    "print(f\"Updated x: {x.numpy()}\")\n",
    "\n",
    "# Update the value of y using assign_add\n",
    "y.assign_add(5)\n",
    "print(f\"Updated y: {y.numpy()}\")\n",
    "\n",
    "# Perform the addition again with the updated values\n",
    "sum_result_2 = tf.add(x, y)\n",
    "print(f\"Sum (updated): {sum_result_2.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f5483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of tensors: 40\n",
      "Value of z after assignment: 40\n"
     ]
    }
   ],
   "source": [
    "# Define some tf.Tensor values\n",
    "value_x = tf.constant(15, dtype=tf.int32)\n",
    "value_y = tf.constant(25, dtype=tf.int32)\n",
    "\n",
    "# Create tf.Variable (if you need to store and potentially update a state)\n",
    "z = tf.Variable(initial_value=0, dtype=tf.int32)\n",
    "\n",
    "# Perform operations using the tf.Tensor values\n",
    "sum_val = tf.add(value_x, value_y)\n",
    "print(f\"Sum of tensors: {sum_val.numpy()}\")\n",
    "\n",
    "# Update the tf.Variable with the result\n",
    "z.assign(sum_val)\n",
    "print(f\"Value of z after assignment: {z.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488deb61",
   "metadata": {},
   "source": [
    "## MNIST Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e38977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2us/step\n",
      "Shape of training images: (60000, 28, 28)\n",
      "Shape of training labels: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# This is the correct way in TensorFlow 2.x\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Now you have your training and testing data as NumPy arrays\n",
    "print(\"Shape of training images:\", train_images.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e0e2dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89d36ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa825cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3567ff290>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dfcc82",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5483a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6130bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_samples = train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5caa52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = train_images.shape[1]*train_images.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "960095db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37008227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron (MLP) model.\n",
    "\n",
    "    This function defines a simple MLP with two hidden layers and an output layer.\n",
    "    It applies affine transformations (matrix multiplication and bias addition)\n",
    "    followed by ReLU activation for the hidden layers and a final affine\n",
    "    transformation for the output layer.\n",
    "\n",
    "    Args:\n",
    "        x: A TensorFlow tensor of shape (batch_size, n_input) representing the input data.\n",
    "        weights: A dictionary containing TensorFlow variables for the weights of each layer.\n",
    "        It should have keys 'h1' (for the first hidden layer), 'h2' (for the second\n",
    "        hidden layer), and 'out' (for the output layer). The shapes should be:\n",
    "            - weights['h1']: (n_input, n_hidden_1)\n",
    "            - weights['h2']: (n_hidden_1, n_hidden_2)\n",
    "            - weights['out']: (n_hidden_2, n_classes)\n",
    "        biases: A dictionary containing TensorFlow variables for the biases of each layer.\n",
    "        It should have keys 'b1' (for the first hidden layer), 'b2' (for the second\n",
    "    hidden layer), and 'out' (for the output layer). The shapes should be:\n",
    "        - biases['b1']: (n_hidden_1,)\n",
    "        - biases['b2']: (n_hidden_2,)\n",
    "        - biases['out']: (n_classes,)\n",
    "\n",
    "    Returns:\n",
    "    A TensorFlow tensor of shape (batch_size, n_classes) representing the output\n",
    "    of the MLP.\n",
    "    \"\"\"\n",
    "    # Hidden fully connected layer 1 with ReLU activation\n",
    "    # X * W + B\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Relu(X * W + B)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # Hidden fully connected layer 2 with ReLU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output fully connected layer with no activation (or you might use softmax later)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c42968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1':tf.Variable(initial_value=tf.random.normal([n_input, n_hidden_1])),\n",
    "    'h2':tf.Variable(initial_value=tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'out':tf.Variable(initial_value=tf.random.normal([n_hidden_2, n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f863cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for hidden layer 1: (784, 256)\n",
      "Weights for hidden layer 2: (256, 256)\n",
      "Weights for output layer: (256, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights for hidden layer 1:\", weights['h1'].shape)\n",
    "print(\"Weights for hidden layer 2:\", weights['h2'].shape)\n",
    "print(\"Weights for output layer:\", weights['out'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7565bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1':tf.Variable(initial_value=tf.random.normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(initial_value=tf.random.normal([n_hidden_2])),\n",
    "    'out':tf.Variable(initial_value=tf.random.normal([n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06cda412",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flat = train_images.reshape(-1, n_input).astype('float32') / 255.0\n",
    "x = tf.constant(train_images_flat[:batch_size])\n",
    "y = tf.one_hot(train_labels, depth=n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c39b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "276e58df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 20:29:34.721288: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: logits and labels must be broadcastable: logits_size=[60000,10] labels_size=[128,10]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:GPU:0}} logits and labels must be broadcastable: logits_size=[60000,10] labels_size=[128,10] [Op:SoftmaxCrossEntropyWithLogits] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cost = tf.reduce_mean(\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/IT/ml/practice/venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   5982\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m5983\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: {{function_node __wrapped__SoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:GPU:0}} logits and labels must be broadcastable: logits_size=[60000,10] labels_size=[128,10] [Op:SoftmaxCrossEntropyWithLogits] name: "
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred,y))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd6a77",
   "metadata": {},
   "source": [
    "# The course is based on TensorFlow 1X which is depricated.\n",
    "## Another practice will be added with TensorFlow 2X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52fece",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
